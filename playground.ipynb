{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f53da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./humanoid-tensorboard/PPO_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | 100      |\n",
      "| time/              |          |\n",
      "|    fps             | 1491     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 22.5        |\n",
      "|    ep_rew_mean          | 110         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1269        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018593324 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.1       |\n",
      "|    explained_variance   | -0.00785    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 491         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.37e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23.2        |\n",
      "|    ep_rew_mean          | 114         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1200        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022298787 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.1       |\n",
      "|    explained_variance   | 0.0204      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 575         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 1.24e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.3        |\n",
      "|    ep_rew_mean          | 119         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1169        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025249943 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.1       |\n",
      "|    explained_variance   | 0.0126      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 402         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0774     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.1e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23.9        |\n",
      "|    ep_rew_mean          | 117         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1150        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028011933 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.1       |\n",
      "|    explained_variance   | 0.0147      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 514         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0819     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 1.13e+03    |\n",
      "-----------------------------------------\n",
      "Training finished!\n",
      "Mean reward over rollout: 4.89\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make(\"Humanoid-v4\")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log='./humanoid-tensorboard')\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "print(\"Training finished!\")\n",
    "obs, _ = env.reset()\n",
    "rewards = []\n",
    "for _ in range(1000):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if done or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "print(f\"Mean reward over rollout: {sum(rewards)/len(rewards):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
