diff --git a/playground.ipynb b/playground.ipynb
index fdeff03..bd2b942 100644
--- a/playground.ipynb
+++ b/playground.ipynb
@@ -144,6 +144,16 @@
     "\n",
     "print(f\"Mean reward over rollout: {sum(rewards)/len(rewards):.2f}\")"
    ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "3362d3c2",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from wandb.integration.sb3 import WandbCallback"
+   ]
   }
  ],
  "metadata": {
diff --git a/src/train_humanoid.py b/src/train_humanoid.py
index e7c702a..c33b8ed 100644
--- a/src/train_humanoid.py
+++ b/src/train_humanoid.py
@@ -6,9 +6,16 @@ from gymnasium.wrappers import RecordVideo
 
 from stable_baselines3 import PPO
 from stable_baselines3.common.monitor import Monitor
-from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList
+from stable_baselines3.common.callbacks import (
+    CallbackList,
+    CheckpointCallback,
+    EvalCallback,
+    BaseCallback,
+)
 from stable_baselines3.common.vec_env import DummyVecEnv
 
+import wandb
+from wandb.integration.sb3 import WandbCallback
 
 def make_train_env(env_id: str):
     return Monitor(gym.make(env_id))
@@ -23,6 +30,36 @@ def make_eval_env(env_id: str, video_dir: str):
     )
     return Monitor(wrapped)
 
+class ConsoleLogCallback(BaseCallback):
+    """Aggregate recent rewards and report to stdout without extra env rollouts."""
+
+    def __init__(self, window: int = 500):
+        super().__init__()
+        self.window = max(window, 1)
+        self._recent_rewards: list[float] = []
+        self.print_count = 1
+
+    def _on_step(self) -> bool:
+        infos = self.locals.get("infos", [])
+        for info in infos:
+            episode_info = info.get("episode")
+            if episode_info is None:
+                continue
+            reward = episode_info.get("r")
+            if reward is None:
+                continue
+            self._recent_rewards.append(float(reward))
+            timestep = self.model.num_timesteps
+            if timestep > self.window * self.print_count:
+                avg_reward = sum(self._recent_rewards) / len(self._recent_rewards)
+                print(
+                    f"Time_step: {timestep}, "
+                    f"MeanReward[{len(self._recent_rewards) if self.window > 1 else 1}]: {avg_reward:.2f}"
+                )
+                self._recent_rewards = []
+                self.print_count+=1
+        return True
+
 def main():
     parser = argparse.ArgumentParser()
     parser.add_argument("--env_id", type=str, default="Humanoid-v4")
@@ -33,6 +70,11 @@ def main():
     parser.add_argument("--logdir", type=str, default="runs")
     parser.add_argument("--models_dir", type=str, default="checkpoints")
     parser.add_argument("--videos_dir", type=str, default="videos")
+    parser.add_argument("--print_window", type=int, default=500)
+    parser.add_argument("--wandb_project", type=str, default="Guided_RL")
+    parser.add_argument("--wandb_entity", type=str, default="")
+    parser.add_argument("--wandb_run_name", type=str, default=None)
+
     args = parser.parse_args()
 
     timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
@@ -57,7 +99,7 @@ def main():
         "MlpPolicy",
         train_env,
         device="auto",                 # CPU on laptop, GPU on cluster
-        verbose=1,
+        verbose=0,
     )
 
     # ---- Callbacks ----
@@ -79,7 +121,38 @@ def main():
         render=False,  # rendering handled by RecordVideo wrapper
     )
 
-    callbacks = CallbackList([checkpoint_cb, eval_cb])
+
+    console_cb = ConsoleLogCallback(window=args.print_window)
+    callbacks_list = [checkpoint_cb, eval_cb, console_cb]
+
+    wandb_run = None
+    if args.wandb_project:
+        api_key = os.environ.get("WANDB_API_KEY")
+        if api_key:
+            wandb.login(key=api_key, relogin=True)
+        else:
+            print("‚ö†Ô∏è Set WANDB_API_KEY in your env for non-interactive wandb auth.")
+            wandb.login()
+        wandb_run = wandb.init(
+            project=args.wandb_project,
+            # entity=args.wandb_entity,
+            name=args.wandb_run_name or timestamp,
+            config={
+                "env_id": args.env_id,
+                "total_timesteps": args.total_timesteps,
+                "checkpoint_interval": args.checkpoint_interval,
+                "eval_freq": args.eval_freq,
+                "eval_episodes": args.eval_episodes,
+                "print_window": args.print_window,
+            },
+            monitor_gym=True,
+            save_code=True,
+        )
+        callbacks_list.append(
+            WandbCallback(model_save_path=None, gradient_save_freq=0, verbose=0)
+        )
+
+    callbacks = CallbackList(callbacks_list)
 
     # ---- Train ----
     model.learn(total_timesteps=args.total_timesteps, callback=callbacks)
@@ -87,6 +160,8 @@ def main():
     # ---- Final save ----
     final_path = os.path.join(models_dir, "final_policy")
     model.save(final_path)
+    if wandb_run is not None:
+        wandb_run.finish()
     print(f"‚úÖ Finished. Final model saved to: {final_path}")
     print(f"üìÅ Run directory: {run_dir}")
     print(f"üìº Videos saved in: {videos_dir}")
